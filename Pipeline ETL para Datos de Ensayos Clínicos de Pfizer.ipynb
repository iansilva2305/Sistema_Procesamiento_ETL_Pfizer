{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalar los paquetes necesarios\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein  # Opcional, pero acelera fuzzywuzzy\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "cNorNler0H3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s09XNK9kzBvT",
        "outputId": "01a00155-8c06-4ed1-9c4c-afe009730b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Pipeline ETL para Datos de Ensayos Clínicos de Pfizer\n",
        "# Implementación de técnicas de limpieza y transformación\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import re\n",
        "from fuzzywuzzy import fuzz, process\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Download the 'stopwords' dataset\n",
        "nltk.download('stopwords')\n",
        "# Download the 'punkt' dataset for tokenization\n",
        "nltk.download('punkt') # Add this line to download Punkt Tokenizer Model\n",
        "# Download the 'punkt_tab' model\n",
        "nltk.download('punkt_tab') # Add this line to download Punkt Tokenizer Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CARGA DE DATOS (Simulada para este ejemplo)\n",
        "def cargar_datos_clinicos(ruta_archivo):\n",
        "    \"\"\"\n",
        "    Carga datos de ensayos clínicos desde múltiples fuentes\n",
        "    \"\"\"\n",
        "    print(f\"Cargando datos desde {ruta_archivo}\")\n",
        "    # En un caso real, conectaríamos con múltiples fuentes\n",
        "    # Para este ejemplo, simulamos con datos de muestra\n",
        "    data = pd.DataFrame({\n",
        "        'patient_id': ['P'+str(i) for i in range(1, 101)],\n",
        "        'age': np.random.normal(55, 15, 100),\n",
        "        'gender': np.random.choice(['M', 'F', 'Male', 'Female', 'm', 'f'], 100),\n",
        "        'treatment_group': np.random.choice(['A', 'B', 'Placebo', 'Control'], 100),\n",
        "        'baseline_measure': np.random.normal(120, 20, 100),\n",
        "        'week_4_measure': np.random.normal(110, 25, 100),\n",
        "        'week_8_measure': np.random.normal(105, 30, 100),\n",
        "        'adverse_event': np.random.choice(['None', 'Mild', 'Moderate', 'Severe', None], 100),\n",
        "        'site_id': np.random.choice(['US-001', 'US-002', 'EU-001', 'EU-002', 'ASIA-001'], 100),\n",
        "        'notes': [f\"Patient {i} exhibited standard response\" if i % 10 != 0 else None for i in range(1, 101)]\n",
        "    })\n",
        "\n",
        "    # Introducimos valores faltantes simulados\n",
        "    for col in ['week_4_measure', 'week_8_measure', 'adverse_event']:\n",
        "        missing_idx = np.random.choice(data.index, size=int(len(data)*0.15), replace=False)\n",
        "        data.loc[missing_idx, col] = np.nan\n",
        "\n",
        "    # Introducimos outliers simulados\n",
        "    outlier_idx = np.random.choice(data.index, size=5, replace=False)\n",
        "    data.loc[outlier_idx, 'week_8_measure'] = data.loc[outlier_idx, 'week_8_measure'] * 3\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "JrGIQ9OxzWwc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LIMPIEZA Y NORMALIZACIÓN\n",
        "def limpiar_normalizar_datos(df):\n",
        "    \"\"\"\n",
        "    Implementa técnicas de limpieza y normalización para datos clínicos\n",
        "    \"\"\"\n",
        "    print(\"Iniciando limpieza y normalización de datos...\")\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # 2.1 Normalización de categorías (ej: género)\n",
        "    gender_mapping = {\n",
        "        'M': 'Male', 'm': 'Male', 'MALE': 'Male',\n",
        "        'F': 'Female', 'f': 'Female', 'FEMALE': 'Female'\n",
        "    }\n",
        "    df_clean['gender'] = df_clean['gender'].replace(gender_mapping)\n",
        "\n",
        "    # 2.2 Detección y manejo de outliers en medidas clínicas\n",
        "    cols_to_check = ['baseline_measure', 'week_4_measure', 'week_8_measure']\n",
        "\n",
        "    for col in cols_to_check:\n",
        "        # Calculamos límites usando el método IQR (más robusto que Z-score)\n",
        "        Q1 = df_clean[col].quantile(0.25)\n",
        "        Q3 = df_clean[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 3 * IQR\n",
        "        upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "        # Identificamos outliers\n",
        "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)].index\n",
        "\n",
        "        print(f\"Detectados {len(outliers)} outliers en {col}\")\n",
        "\n",
        "        # Marcamos outliers con flag (no eliminamos, importante en datos clínicos)\n",
        "        df_clean[f'{col}_is_outlier'] = False\n",
        "        df_clean.loc[outliers, f'{col}_is_outlier'] = True\n",
        "\n",
        "        # Opcionalmente, winsorización (recorte) para análisis estadísticos\n",
        "        df_clean[f'{col}_winsorized'] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    # 2.3 Imputación de valores faltantes usando KNN (más adecuado para datos clínicos)\n",
        "    cols_for_imputation = ['week_4_measure', 'week_8_measure']\n",
        "    # Preparamos variables para el modelo de imputación\n",
        "    features_for_imputation = df_clean[['age', 'baseline_measure']].copy()\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features_for_imputation)\n",
        "\n",
        "    # Aplicamos KNN imputation\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    df_imputed = df_clean.copy()\n",
        "    df_imputed[cols_for_imputation] = imputer.fit_transform(\n",
        "        np.hstack((features_scaled, df_clean[cols_for_imputation].values))\n",
        "    )[:, 2:]\n",
        "\n",
        "    # Marcamos valores imputados\n",
        "    for col in cols_for_imputation:\n",
        "        df_imputed[f'{col}_imputed'] = df_clean[col].isna()\n",
        "\n",
        "    # 2.4 Normalización de identificadores de sitios\n",
        "    site_pattern = re.compile(r'([A-Za-z]+)-(\\d+)')\n",
        "\n",
        "    def normalize_site_id(site_id):\n",
        "        match = site_pattern.match(site_id)\n",
        "        if match:\n",
        "            region, number = match.groups()\n",
        "            return f\"{region.upper()}-{number.zfill(3)}\"\n",
        "        return site_id\n",
        "\n",
        "    df_imputed['site_id'] = df_imputed['site_id'].apply(normalize_site_id)\n",
        "\n",
        "    # 2.5 Tratamiento de texto en notas clínicas\n",
        "    def clean_text(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        # Tokenización y eliminación de stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "        return \" \".join(filtered_tokens)\n",
        "\n",
        "    df_imputed['notes_cleaned'] = df_imputed['notes'].apply(clean_text)\n",
        "\n",
        "    return df_imputed"
      ],
      "metadata": {
        "id": "ciimG08Ezhfq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. TRANSFORMACIÓN Y FEATURE ENGINEERING\n",
        "def transformar_datos(df):\n",
        "    \"\"\"\n",
        "    Implementa transformaciones avanzadas y generación de nuevas variables\n",
        "    \"\"\"\n",
        "    print(\"Iniciando transformación y feature engineering...\")\n",
        "    df_transform = df.copy()\n",
        "\n",
        "    # 3.1 Generación de variables derivadas clínicamente relevantes\n",
        "    # Cambio absoluto y relativo desde línea base\n",
        "    df_transform['change_week4'] = df_transform['week_4_measure'] - df_transform['baseline_measure']\n",
        "    df_transform['change_week8'] = df_transform['week_8_measure'] - df_transform['baseline_measure']\n",
        "\n",
        "    df_transform['pct_change_week4'] = (df_transform['change_week4'] / df_transform['baseline_measure']) * 100\n",
        "    df_transform['pct_change_week8'] = (df_transform['change_week8'] / df_transform['baseline_measure']) * 100\n",
        "\n",
        "    # 3.2 Clasificación de respondedores según criterios clínicos\n",
        "    # (Simulamos un criterio típico en ensayos clínicos: >15% de mejora)\n",
        "    df_transform['responder_status'] = np.where(df_transform['pct_change_week8'] < -15, 'Responder', 'Non-Responder')\n",
        "\n",
        "    # 3.3 Agregación de eventos adversos (para análisis de seguridad)\n",
        "    adverse_severity_mapping = {\n",
        "        'None': 0,\n",
        "        'Mild': 1,\n",
        "        'Moderate': 2,\n",
        "        'Severe': 3\n",
        "    }\n",
        "    df_transform['adverse_event_score'] = df_transform['adverse_event'].map(adverse_severity_mapping).fillna(0)\n",
        "\n",
        "    # 3.4 Codificación de variables categóricas para modelado\n",
        "    # One-hot encoding para variables nominales\n",
        "    categorical_cols = ['gender', 'treatment_group', 'site_id']\n",
        "    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "    encoded_data = encoder.fit_transform(df_transform[categorical_cols])\n",
        "\n",
        "    # Convertimos a DataFrame con nombres apropiados\n",
        "    encoded_cols = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_cols, index=df_transform.index)\n",
        "\n",
        "    # Combinamos con el dataframe original\n",
        "    df_transform = pd.concat([df_transform, encoded_df], axis=1)\n",
        "\n",
        "    # 3.5 Normalización de variables continuas para análisis y visualización\n",
        "    numeric_cols = ['age', 'baseline_measure', 'week_4_measure', 'week_8_measure',\n",
        "                   'change_week4', 'change_week8']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    df_transform[['scaled_' + col for col in numeric_cols]] = scaler.fit_transform(df_transform[numeric_cols])\n",
        "\n",
        "    # 3.6 Variables de agrupación para análisis estratificado\n",
        "    df_transform['age_group'] = pd.cut(df_transform['age'],\n",
        "                                      bins=[0, 40, 65, 100],\n",
        "                                      labels=['<40', '40-65', '>65'])\n",
        "\n",
        "    # Extracción de región del site_id\n",
        "    df_transform['region'] = df_transform['site_id'].str.split('-').str[0]\n",
        "\n",
        "    return df_transform"
      ],
      "metadata": {
        "id": "5qXIy4otzoty"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. VALIDACIÓN DE CALIDAD DE DATOS\n",
        "def validar_calidad_datos(df_original, df_procesado):\n",
        "    \"\"\"\n",
        "    Genera métricas de calidad del proceso ETL\n",
        "    \"\"\"\n",
        "    print(\"Generando informe de calidad de datos...\")\n",
        "\n",
        "    # 4.1 Completitud de datos\n",
        "    missing_before = df_original.isna().sum().sum()\n",
        "    missing_after = df_procesado[[col for col in df_original.columns]].isna().sum().sum()\n",
        "    pct_improvement = ((missing_before - missing_after) / missing_before) * 100 if missing_before > 0 else 0\n",
        "\n",
        "    # 4.2 Detección de outliers\n",
        "    outlier_cols = [col for col in df_procesado.columns if col.endswith('_is_outlier')]\n",
        "    outliers_detected = df_procesado[outlier_cols].sum().sum()\n",
        "\n",
        "    # 4.3 Valores imputados\n",
        "    imputed_cols = [col for col in df_procesado.columns if col.endswith('_imputed')]\n",
        "    values_imputed = df_procesado[imputed_cols].sum().sum()\n",
        "\n",
        "    # Generar reporte\n",
        "    quality_report = {\n",
        "        'registros_originales': len(df_original),\n",
        "        'variables_originales': len(df_original.columns),\n",
        "        'variables_generadas': len(df_procesado.columns) - len(df_original.columns),\n",
        "        'valores_faltantes_original': missing_before,\n",
        "        'valores_faltantes_procesado': missing_after,\n",
        "        'mejora_completitud': f\"{pct_improvement:.2f}%\",\n",
        "        'outliers_detectados': outliers_detected,\n",
        "        'valores_imputados': values_imputed\n",
        "    }\n",
        "\n",
        "    return quality_report"
      ],
      "metadata": {
        "id": "C6dy_YsQzum9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. PIPELINE COMPLETO DE ETL\n",
        "def ejecutar_pipeline_completo(ruta_datos):\n",
        "    \"\"\"\n",
        "    Ejecuta el pipeline completo de ETL para datos clínicos de Pfizer\n",
        "    \"\"\"\n",
        "    print(\"Iniciando pipeline ETL para datos de Pfizer...\")\n",
        "\n",
        "    # Paso 1: Carga de datos\n",
        "    df_original = cargar_datos_clinicos(ruta_datos)\n",
        "    print(f\"Datos cargados: {len(df_original)} registros, {len(df_original.columns)} variables\")\n",
        "\n",
        "    # Paso 2: Limpieza y normalización\n",
        "    df_limpio = limpiar_normalizar_datos(df_original)\n",
        "    print(f\"Datos limpios: {len(df_limpio)} registros, {len(df_limpio.columns)} variables\")\n",
        "\n",
        "    # Paso 3: Transformación y feature engineering\n",
        "    df_transformado = transformar_datos(df_limpio)\n",
        "    print(f\"Datos transformados: {len(df_transformado)} registros, {len(df_transformado.columns)} variables\")\n",
        "\n",
        "    # Paso 4: Validación de calidad\n",
        "    reporte_calidad = validar_calidad_datos(df_original, df_transformado)\n",
        "    print(\"\\nREPORTE DE CALIDAD DEL PROCESO ETL:\")\n",
        "    for k, v in reporte_calidad.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    # Exportar resultados (en un caso real, cargaríamos a un data warehouse)\n",
        "    # df_transformado.to_csv(\"datos_procesados_pfizer.csv\", index=False)\n",
        "\n",
        "    return df_transformado, reporte_calidad"
      ],
      "metadata": {
        "id": "LQyFv6Uez085"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecución del pipeline (simulado para demostración)\n",
        "if __name__ == \"__main__\":\n",
        "    datos_procesados, reporte = ejecutar_pipeline_completo(\"/content/datos_ensayos_clinicos.csv\")\n",
        "    print(\"\\nPipeline ETL completado exitosamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxfs9BTKz5FK",
        "outputId": "5ce5df92-5512-43ea-8583-191d9725850c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando pipeline ETL para datos de Pfizer...\n",
            "Cargando datos desde /content/datos_ensayos_clinicos.csv\n",
            "Datos cargados: 100 registros, 10 variables\n",
            "Iniciando limpieza y normalización de datos...\n",
            "Detectados 0 outliers en baseline_measure\n",
            "Detectados 0 outliers en week_4_measure\n",
            "Detectados 4 outliers en week_8_measure\n",
            "Datos limpios: 100 registros, 19 variables\n",
            "Iniciando transformación y feature engineering...\n",
            "Datos transformados: 100 registros, 41 variables\n",
            "Generando informe de calidad de datos...\n",
            "\n",
            "REPORTE DE CALIDAD DEL PROCESO ETL:\n",
            "registros_originales: 100\n",
            "variables_originales: 10\n",
            "variables_generadas: 31\n",
            "valores_faltantes_original: 71\n",
            "valores_faltantes_procesado: 41\n",
            "mejora_completitud: 42.25%\n",
            "outliers_detectados: 4\n",
            "valores_imputados: 30\n",
            "\n",
            "Pipeline ETL completado exitosamente\n"
          ]
        }
      ]
    }
  ]
}